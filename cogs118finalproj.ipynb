{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e432599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEEDS = [0, 1, 2]\n",
    "N_JOBS = -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e6a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c9dd07",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: Adult Income (adult_income) ===\n",
      "\n",
      "  Train size = 0.20\n",
      "    Trial 1/3\n",
      "      Model: log_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(70997) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70998) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(70999) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71001) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71004) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71005) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(71006) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model: decision_tree\n",
      "      Model: knn\n",
      "    Trial 2/3\n",
      "      Model: log_reg\n",
      "      Model: decision_tree\n",
      "      Model: knn\n",
      "    Trial 3/3\n",
      "      Model: log_reg\n",
      "      Model: decision_tree\n",
      "      Model: knn\n",
      "\n",
      "  Train size = 0.50\n",
      "    Trial 1/3\n",
      "      Model: log_reg\n",
      "      Model: decision_tree\n",
      "      Model: knn\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COGS 118A Final Project: Interpretable Models Across Datasets\n",
    "# Datasets: Adult, Breast Cancer, SMS Spam, HAR, Digits\n",
    "# Models: Logistic Regression, Decision Tree, KNN\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global settings\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_TRIALS = 3          # You can reduce to 1 or 2 if runtime is an issue\n",
    "TRAIN_SIZES = [0.2, 0.5, 0.8]  # Proportion of data used for training\n",
    "\n",
    "DATASET_NAMES = [\n",
    "    \"adult_income\",\n",
    "    \"breast_cancer\",\n",
    "    \"sms_spam\",\n",
    "    \"har_smartphone\",\n",
    "    \"digits\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset loaders & binary label definitions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def load_adult_income():\n",
    "    \"\"\"\n",
    "    Adult / Census Income dataset (UCI id=2).\n",
    "    Task: predict whether income > 50K (1) vs <= 50K (0).\n",
    "    \"\"\"\n",
    "    ds = fetch_ucirepo(id=2)\n",
    "    X = ds.data.features.copy()\n",
    "    targets = ds.data.targets\n",
    "\n",
    "    # Targets can be a Series or DataFrame\n",
    "    if isinstance(targets, pd.DataFrame):\n",
    "        y_raw = targets.iloc[:, 0].astype(str)\n",
    "    else:\n",
    "        y_raw = targets.astype(str)\n",
    "\n",
    "    # Positive class: contains '>50K'\n",
    "    y = y_raw.str.contains('>50K').astype(int)\n",
    "\n",
    "    info = {\n",
    "        \"name\": \"Adult Income\",\n",
    "        \"is_text\": False\n",
    "    }\n",
    "    return X, y, info\n",
    "\n",
    "\n",
    "def load_breast_cancer():\n",
    "    \"\"\"\n",
    "    Breast Cancer Wisconsin (Diagnostic) (UCI id=17).\n",
    "    Task: Malignant (1) vs Benign (0).\n",
    "    \"\"\"\n",
    "    ds = fetch_ucirepo(id=17)\n",
    "    X = ds.data.features.copy()\n",
    "    targets = ds.data.targets\n",
    "\n",
    "    if isinstance(targets, pd.DataFrame):\n",
    "        y_raw = targets.iloc[:, 0].astype(str)\n",
    "    else:\n",
    "        y_raw = targets.astype(str)\n",
    "\n",
    "    # Typically 'M' and 'B'\n",
    "    y = (y_raw == 'M').astype(int)\n",
    "\n",
    "    info = {\n",
    "        \"name\": \"Breast Cancer (Diagnostic)\",\n",
    "        \"is_text\": False\n",
    "    }\n",
    "    return X, y, info\n",
    "\n",
    "\n",
    "def load_sms_spam():\n",
    "    \"\"\"\n",
    "    SMS Spam Collection (UCI id=228).\n",
    "    Task: spam (1) vs ham (0).\n",
    "    The features DataFrame has one text column; we just use the first column.\n",
    "    \"\"\"\n",
    "    ds = fetch_ucirepo(id=228)\n",
    "    X = ds.data.features.copy()\n",
    "    targets = ds.data.targets\n",
    "\n",
    "    # X is a DataFrame with one text column.\n",
    "    text_series = X.iloc[:, 0].astype(str)\n",
    "\n",
    "    if isinstance(targets, pd.DataFrame):\n",
    "        y_raw = targets.iloc[:, 0].astype(str)\n",
    "    else:\n",
    "        y_raw = targets.astype(str)\n",
    "\n",
    "    y = (y_raw.str.lower() == \"spam\").astype(int)\n",
    "\n",
    "    info = {\n",
    "        \"name\": \"SMS Spam\",\n",
    "        \"is_text\": True,\n",
    "        \"text_series\": text_series  # store for convenience\n",
    "    }\n",
    "    # Return X as the raw text Series to keep types simple in the experiment loop\n",
    "    return text_series, y, info\n",
    "\n",
    "\n",
    "def load_har():\n",
    "    \"\"\"\n",
    "    Human Activity Recognition Using Smartphones (UCI id=240).\n",
    "    Original task: 6-way activity classification.\n",
    "    Here we make it binary:\n",
    "        Positive (1): WALKING / WALKING_UPSTAIRS / WALKING_DOWNSTAIRS\n",
    "        Negative (0): SITTING / STANDING / LAYING\n",
    "    \"\"\"\n",
    "    ds = fetch_ucirepo(id=240)\n",
    "    X = ds.data.features.copy()\n",
    "    targets = ds.data.targets\n",
    "\n",
    "    if isinstance(targets, pd.DataFrame):\n",
    "        y_raw = targets.iloc[:, 0].astype(str)\n",
    "    else:\n",
    "        y_raw = targets.astype(str)\n",
    "\n",
    "    dynamic = [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\"]\n",
    "    y = y_raw.isin(dynamic).astype(int)\n",
    "\n",
    "    info = {\n",
    "        \"name\": \"HAR (Dynamic vs Static)\",\n",
    "        \"is_text\": False\n",
    "    }\n",
    "    return X, y, info\n",
    "\n",
    "\n",
    "def load_digits():\n",
    "    \"\"\"\n",
    "    Optical Recognition of Handwritten Digits (UCI id=80).\n",
    "    Original task: 10 digits (0–9).\n",
    "    Here we make a simple binary task:\n",
    "        Positive class (1): digits 5–9\n",
    "        Negative class (0): digits 0–4\n",
    "    \"\"\"\n",
    "    ds = fetch_ucirepo(id=80)\n",
    "    X = ds.data.features.copy()\n",
    "    targets = ds.data.targets\n",
    "\n",
    "    if isinstance(targets, pd.DataFrame):\n",
    "        y_raw = targets.iloc[:, 0].astype(int)\n",
    "    else:\n",
    "        y_raw = targets.astype(int)\n",
    "\n",
    "    y = (y_raw >= 5).astype(int)\n",
    "\n",
    "    info = {\n",
    "        \"name\": \"Digits (5–9 vs 0–4)\",\n",
    "        \"is_text\": False\n",
    "    }\n",
    "    return X, y, info\n",
    "\n",
    "\n",
    "def load_dataset_by_name(name):\n",
    "    if name == \"adult_income\":\n",
    "        return load_adult_income()\n",
    "    elif name == \"breast_cancer\":\n",
    "        return load_breast_cancer()\n",
    "    elif name == \"sms_spam\":\n",
    "        return load_sms_spam()\n",
    "    elif name == \"har_smartphone\":\n",
    "        return load_har()\n",
    "    elif name == \"digits\":\n",
    "        return load_digits()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {name}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model definitions & hyperparameter grids\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def get_models_and_grids():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        models: dict[name -> base estimator type (class)]\n",
    "        param_grids: dict[name -> param grid for GridSearchCV]\n",
    "    NOTE: we only tune clf__* params so this works with any preprocessing.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"log_reg\": LogisticRegression(max_iter=1000),\n",
    "        \"decision_tree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        \"knn\": KNeighborsClassifier()\n",
    "    }\n",
    "\n",
    "    param_grids = {\n",
    "        \"log_reg\": {\n",
    "            \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__solver\": [\"liblinear\"]  # good for small/medium datasets\n",
    "        },\n",
    "        \"decision_tree\": {\n",
    "            \"clf__max_depth\": [None, 5, 10, 20],\n",
    "            \"clf__min_samples_leaf\": [1, 5, 10]\n",
    "        },\n",
    "        \"knn\": {\n",
    "            \"clf__n_neighbors\": [3, 5, 11],\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return models, param_grids\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Preprocessing builders\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def build_tabular_preprocessor(X_train):\n",
    "    \"\"\"\n",
    "    Build a ColumnTransformer for mixed numeric/categorical tabular data.\n",
    "    Works for Adult, Breast Cancer, HAR, Digits.\n",
    "    \"\"\"\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = [c for c in X_train.columns if c not in numeric_cols]\n",
    "\n",
    "    numeric_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipeline, numeric_cols),\n",
    "            (\"cat\", categorical_pipeline, categorical_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def build_text_preprocessor():\n",
    "    \"\"\"\n",
    "    Preprocessor for SMS Spam (raw text).\n",
    "    Tfidf -> TruncatedSVD (to keep dimensionality manageable).\n",
    "    \"\"\"\n",
    "    text_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=5000)),\n",
    "            (\"svd\", TruncatedSVD(n_components=100, random_state=RANDOM_STATE))\n",
    "        ]\n",
    "    )\n",
    "    return text_pipeline\n",
    "\n",
    "\n",
    "def build_pipeline(model_name, base_model, is_text, X_train=None):\n",
    "    \"\"\"\n",
    "    Build a sklearn Pipeline that ends in 'clf'.\n",
    "\n",
    "    For tabular datasets:\n",
    "        [ColumnTransformer] -> [clf]\n",
    "\n",
    "    For text dataset:\n",
    "        [Tfidf + SVD] -> [clf]\n",
    "    \"\"\"\n",
    "    if is_text:\n",
    "        text_pre = build_text_preprocessor()\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocess\", text_pre),\n",
    "                (\"clf\", base_model)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        if X_train is None:\n",
    "            raise ValueError(\"X_train must be provided for tabular data.\")\n",
    "        tab_pre = build_tabular_preprocessor(X_train)\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocess\", tab_pre),\n",
    "                (\"clf\", base_model)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Experiment loop\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def run_experiments():\n",
    "    models, param_grids = get_models_and_grids()\n",
    "    all_results = []\n",
    "\n",
    "    for dataset_key in DATASET_NAMES:\n",
    "        X_raw, y, info = load_dataset_by_name(dataset_key)\n",
    "        print(f\"\\n=== Dataset: {info['name']} ({dataset_key}) ===\")\n",
    "        is_text = info[\"is_text\"]\n",
    "\n",
    "        # For text dataset, X_raw is a Series of strings\n",
    "        if is_text:\n",
    "            X_array = X_raw.values  # 1D array of texts\n",
    "        else:\n",
    "            X_array = X_raw  # DataFrame\n",
    "\n",
    "        for train_size in TRAIN_SIZES:\n",
    "            print(f\"\\n  Train size = {train_size:.2f}\")\n",
    "\n",
    "            splitter = StratifiedShuffleSplit(\n",
    "                n_splits=N_TRIALS,\n",
    "                train_size=train_size,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "\n",
    "            for trial_idx, (train_idx, test_idx) in enumerate(\n",
    "                splitter.split(X_array, y),\n",
    "                start=1\n",
    "            ):\n",
    "                print(f\"    Trial {trial_idx}/{N_TRIALS}\")\n",
    "\n",
    "                if is_text:\n",
    "                    X_train = X_array[train_idx]\n",
    "                    X_test = X_array[test_idx]\n",
    "                else:\n",
    "                    X_train = X_array.iloc[train_idx].copy()\n",
    "                    X_test = X_array.iloc[test_idx].copy()\n",
    "\n",
    "                y_train = y.iloc[train_idx]\n",
    "                y_test = y.iloc[test_idx]\n",
    "\n",
    "                for model_name, base_model in models.items():\n",
    "                    print(f\"      Model: {model_name}\")\n",
    "\n",
    "                    pipe = build_pipeline(\n",
    "                        model_name=model_name,\n",
    "                        base_model=base_model,\n",
    "                        is_text=is_text,\n",
    "                        X_train=None if is_text else X_train\n",
    "                    )\n",
    "                    param_grid = param_grids[model_name]\n",
    "\n",
    "                    grid = GridSearchCV(\n",
    "                        estimator=pipe,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=3,\n",
    "                        scoring=\"accuracy\",\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "\n",
    "                    grid.fit(X_train, y_train)\n",
    "\n",
    "                    # Training & test performance\n",
    "                    y_train_pred = grid.predict(X_train)\n",
    "                    y_test_pred = grid.predict(X_test)\n",
    "\n",
    "                    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "                    val_acc_cv = grid.best_score_\n",
    "                    best_params = grid.best_params_\n",
    "\n",
    "                    result_row = {\n",
    "                        \"dataset_key\": dataset_key,\n",
    "                        \"dataset_name\": info[\"name\"],\n",
    "                        \"is_text\": is_text,\n",
    "                        \"train_size\": train_size,\n",
    "                        \"trial\": trial_idx,\n",
    "                        \"model\": model_name,\n",
    "                        \"train_acc\": train_acc,\n",
    "                        \"val_acc_cv\": val_acc_cv,\n",
    "                        \"test_acc\": test_acc\n",
    "                    }\n",
    "\n",
    "                    # Flatten best_params into the result row\n",
    "                    for k, v in best_params.items():\n",
    "                        result_row[f\"best_{k}\"] = v\n",
    "\n",
    "                    all_results.append(result_row)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main: run and summarize\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = run_experiments()\n",
    "\n",
    "    # Save raw results (every trial, every split)\n",
    "    results_df.to_csv(\"all_results_raw.csv\", index=False)\n",
    "\n",
    "    # Aggregate: mean accuracies over trials\n",
    "    summary = (\n",
    "        results_df\n",
    "        .groupby([\"dataset_key\", \"dataset_name\", \"model\", \"train_size\"])\n",
    "        [[\"train_acc\", \"val_acc_cv\", \"test_acc\"]]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summary.to_csv(\"summary_results.csv\", index=False)\n",
    "\n",
    "    print(\"\\n=== Summary (first few rows) ===\")\n",
    "    print(summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95330d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs118a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
